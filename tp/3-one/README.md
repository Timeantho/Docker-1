# TP3 : Self-hosted private cloud platform

Dans ce TP, **on d√©ploie notre propre plateforme de cloud priv√©.**

**L'id√©e est d'arriver √† proposer des fonctionnalit√©s similaires √† ce que peut proposer Azure ou AWS.** Bien s√ªr, on restera dans un setup simpliste, et on sera (tr√®s) loin de proposer des choses aussi int√©gr√©es que AWS ou Azure.

Le but ici est de mettre en place une plateforme extensible permettant de cr√©er des VMs : on veut un outil qui permet de cr√©er des VMs "√† volont√©", en se basant sur plusieurs hyperviseurs.

Tout ce qui est de l'ordre **la virtualisation, du stockage, et du r√©seau doivent √™tre compl√®tement abstraits** : on veut faire 3 clics et √ßa cr√©e 10 VMs.

> *On va faire √ßa sur vos postes, donc c'est un TP qui n√©cessite au minimum de minimum 8Go de RAM. Les gars on va pas se le cacher, monter une plateforme cloud √ßa se fait pas avec un ch√¢teau de sable.*

---

**L'outil qu'on va utiliser pour setup tout √ßa c'est Open Nebula.** Parce que c'est libre et open source, et parce que je l'aime bien. Y'aurait aussi OpenStack, plus r√©put√©, mais c'est beaucoup moins KISS (Keep It Simple Stupid), et donc forc√©ment, √ßa tourne genre pas trop sur nos ptits PCs.

![One](./img/one.png)

Open Nebula, il repose sur des trucs super **standards** pour tout faire :

- c√¥t√© hyperviseurs, on va utiliser **KVM**
- c√¥t√© r√©seau, √ßa va √™tre du **bridge Linux + un peu de FirewallD + du VXLAN**
- c√¥t√© stockage, on va rester simple avec du stockage local
- pour la communication entre les noeuds, c'est du **SSH**

## Sommaire

- [TP3 : Self-hosted private cloud platform](#tp3--self-hosted-private-cloud-platform)
  - [Sommaire](#sommaire)
- [0. Pr√©requis](#0-pr√©requis)
- [I. Pr√©sentation du lab](#i-pr√©sentation-du-lab)
  - [1. Architecture](#1-architecture)
  - [2. Noeud Frontend](#2-noeud-frontend)
  - [3. Noeuds KVM](#3-noeuds-kvm)
- [II. Setup](#ii-setup)
  - [1. Frontend](#1-frontend)
  - [2. Noeuds KVM](#2-noeuds-kvm)
  - [3. R√©seau](#3-r√©seau)
- [III. Utiliser la plateforme](#iii-utiliser-la-plateforme)
- [IV. Ajouter d'un noeud et VXLAN](#iv-ajouter-dun-noeud-et-vxlan)

# 0. Pr√©requis

‚ûú **VM Rocky pr√™te √† √™tre clon√©e**

- en local dans votre VBox
- on va avoir besoin de 3 machines :
  - une VM avec 
    - au moins 2G de RAM (3/4 conseill√©) et 1 CPU
    - un acc√®s √† internet + acc√®s √† un LAN avec les autres VMs
  - deux autres VMs avec 
    - au moins 2G de RAM (4/6/8 conseill√©s) et 1 (ou 2) CPUs
    - un acc√®s √† internet + acc√®s √† un LAN avec les autres VMs
- ptite pr√©paration must-have sur toutes les VMs :


```bash
# update de tout le syst√®me
dnf update -y

# ajout de paquets utiles, n'h√©sitez pas √† ajouter les v√¥tres
dnf install -y vim

# d√©sactivation de SELinux
sed -i 's/SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config
setenforce 0
```

‚ûú **Activer la Nested virtu sur les deux VMs identiques**

"Nested virtualization" √ßa d√©signe le fait de faire une VM dans une VM. Bah ouais, le but de la plateforme qu'on va monter, c'est de lancer des VMs, et on va tester √ßa dans des VMs.

Il y a une option dans les settings de VirtualBox pour chaque VM : `Nested VT-x/AMD-V` en anglais.

![Inception](./img/inception.png)

‚ûú Ce TP n'est **qu'une paraphrase gigantesque de [la doc officielle](https://docs.opennebula.io/6.10/)**

‚ûú **Comprendre un minimum le fonctionnement/l'architecture de OpenNebula** avant de commencer, la doc explique plut√¥t bien en intro ! Ou √©coutez moi en cours sinon :d


# I. Pr√©sentation du lab

## 1. Architecture

| Node           | IP Address  | R√¥le                         |
|----------------|-------------|------------------------------|
| `frontend.one` | `10.3.1.10` | WebUI OpenNebula             |
| `kvm1.one`     | `10.3.1.11` | Hyperviseur + Endpoint VXLAN |
| `kvm2.one`     | `10.3.1.12` | Hyperviseur + Endpoint VXLAN |

üåû **Allumez les VMs et effectuez la conf √©l√©mentaire :**

- adresse IP statique
- d√©finition du nom de domaine avec `hostnamectl`
- vous remplirez les fichiers `/etc/hosts` des trois machines pour qu'elles se joignent avec leurs noms
- pour le compte-rendu, j'veux juste :
  - `ping kvm1.one` depuis `frontend.one`
  - `ping kvm2.one` depuis `frontend.one`

> Je vous recommande de n'utiliser que les noms des VMs plut√¥t que leurs IP pour le reste du TP. Il pourrait √™tre malin d'ajouter aussi leurs adresses IP au fichier `hosts` de votre PC ;)

## 2. Noeud Frontend

Le noeud `frontend.one` expose une interface Web sexy (oupa?) qui permet de contr√¥ler la plateforme.

C'est depuis l√† qu'on fera nos premiers pas pour :

- cr√©er un r√©seau
- t√©l√©charger des templates de VMs
- ajouter des noeuds de virtu au cluster (nos noeuds KVM)
- et surtout, cr√©er des VMs !

## 3. Noeuds KVM

Les noeuds `kvm1.one` et `kvm2.one` sont des VMs avec qemu/KVM install√©s, qui √† deux, forment l'hyperviseur natif qu'on utilise sur des syst√®mes Linux.

> On parle souvent juste de "hyperviseur KVM" bien que qemu soit syst√©matiquement pr√©sent dans le mix.

Pour contr√¥ler des VMs KVM, il faut utiliser le d√©mon `libvirtd`. Une fois qu'il est d√©marr√©, on peut l'utiliser pour g√©rer des VMs.

> On ne l'utilisera pas nous-m√™mes dans ce TP : c'est OpenNebula qui s'en servira pour cr√©er des VMs.

L'id√©e ici, dans le contexte du TP, c'est de pr√©parer deux VMs avec l'hyperviseur KVM install√©, et on permettra au noeud `frontend.one` de s'y connecter en SSH pour g√©rer des VMs.

> Ui, OpenNebula repose sur SSH. Simpliste, mais surtout standard et robuste !

# II. Setup

## 1. Frontend

Le noeud `frontend.one` va h√©berger la logique de l'application, et exposer la WebUI ainsi que l'API.

‚ûú **J'ai fait [un doc d√©di√© pour le setup du frontend](./frontend.md), d√©roulez-le en entier sur la machine `frontend.one` avant de continuer.**

## 2. Noeuds KVM

Le noeud `kvm1.one` va h√©berger un hyperviseur KVM. Il sera contr√¥l√© par `frontend.one` (√† travers SSH).

‚ûú **J'ai fait [un doc d√©di√© pour le setup des noeuds KVM](./kvm.md), d√©roulez-le en entier, uniquement sur la machine `kvm1.one`, avant de continuer.**

> On configurera `kvm2.one` plus tard !

## 3. R√©seau

On va cr√©er un r√©seau VXLAN pour que les VMs pourront utiliser pour communiquer.

‚ûú **Pouif, l√† encore, j'ai fait [un doc d√©di√© pour le setup du r√©seau](./network.md), d√©roulez-le en entier, avant de continuer.**

- les commandes ne sont √† effectuer que `kvm1.one`
- le setup de `kvm2.one` ne viendra que dans la partie IV du TP

# III. Utiliser la plateforme

Bah ouais il serait temps nan. Pop des ptites VMs.

OpenNebula fournit des images toutes pr√™tes, ready-to-use, qu'on peut lancer au sein de notre plateforme Cloud.

‚ûú **RDV de nouveau sur la WebUI de OpenNebula, et naviguez dans `Settings > Onglet Auth`**

- OpenNebula a g√©n√©r√© une paire de cl√© sur la machine `frontend.one`
- elle se trouve dans le dossier `.ssh` dans le homedir de l'utilisateur `oneadmin`
- d√©posez la cl√© publique dans cet interface de la WebUI

> *Dans un cas r√©el, on poserait clairement une autre cl√©, la n√¥tre. On pourrait aussi en d√©poser plusieurs, s'il y a plusieurs admins dans la bo√Æte. Ca pourrait se faire avec une image custom et du `cloud-init` par exemple. L√† on fait √ßa comme √ßa, pour pas vous brainfuck avec 14 cl√©s diff√©rentes. Appelez-moi pour un setup propre si vous voulez.*

‚ûú **Toujours sur la WebUI de OpenNebula, naviguez dans `Storage > Apps`**

R√©cup√©rez l'image de Rocky Linux 9 dans cette interface.

> Les images propos√©es par les gars d'OpenNebula, on peut s'y connecter qu'en SSH, il faudra donc pouvoir les joindre niveau IP pour les utiliser.

‚ûú **Toujouuuuurs sur la WebUI de OpenNebula, naviguez dans `Instances > VMs`**

- cr√©ez votre premi√®re VM :
  - doit utiliser l'image Rocky Linux 9 qu'on a cr√©√© pr√©c√©demment
  - doit utiliser le virtual network cr√©√© pr√©c√©demment

‚ûú **Tester la connectivit√© √† la VM**

- d√©j√† est-ce qu'on peut la ping ?
  - depuis le noeud `kvm1.one`, faites un `ping` vers l'IP de la VM
  - l'IP de la VM est visible dans la WebUI
- pour pouvoir se co en SSH, il faut utiliser la cl√© de `oneadmin`, suivez le guide :

```bash
# connectez vous en SSH sur la machine frontend.one

# devenez l'utilisateur oneadmin
[it4@frontend ~]$ sudo su - oneadmin

# lancez un agent SSH (demandez-moi si vous voulez une explication sur √ßa)
[oneadmin@frontend ~]$ eval $(ssh-agent)

# ajoutez la cl√© priv√©e √† l'agent SSH
[oneadmin@frontend ~]$ ssh-add
Identity added: /var/lib/one/.ssh/id_rsa (oneadmin@frontend)

# rebondir sur kvm1 pour se connecter √† la VM qui a l'IP 10.220.220.100
[oneadmin@frontend ~]$ ssh -J kvm1 root@10.220.220.100

# on est co dans la VM
[root@localhost ~]# 
```

‚ûú **Si vous avez bien un shell dans la VM, vous √™tes au bout des p√©rip√©ties, pour un setup basique !**

- vous pouvez √©ventuellement ajouter l'IP de la machine h√¥te comme route par d√©faut pour avoir internet (l'IP du bridge VXLAN de l'h√¥te) :

```bash
[root@localhost ~]# ip route add default via 10.220.220.201
[root@localhost ~]# ping 1.1.1.1
```

# IV. Ajouter d'un noeud et VXLAN

‚ûú [**Et l'dernier ! Doc d√©di√© √† la partie IV.**](./kvm2.md)

![Someone else](./img/someone_else.png)
