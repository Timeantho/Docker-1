# Part III : Terraform

**Dans cette derni√®re partie on va explorer une utilisation basique de Terraform.**

On utilisera Terraform pour automatiser la cr√©ation de machines dans Azure, *via* des fichiers texte avec la syntaxe Terraform.

> Ouais, encore une new syntaxe ! Bah bienvenue dans le monde du cloud/devops toussa : on est des admins qui √©crivons du code (Dockerfile, docker-compose.yml, Terraform, et d'autres) pour d√©ployer des machines et de la conf !

![APPLY](./img/apply.jpg)

Terraform va permettre d'automatiser la cr√©ation de ressources dans Azure, *Resource Group* comme VMs, ou tout autre type de ressource que sait g√©rer Azure.

‚ûú Je vous laisse l√† encore suivre **[la documentation officielle de Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)** pour l'installer sur votre poste

# Sommaire

- [Part III : Terraform](#part-iii--terraform)
- [Sommaire](#sommaire)
  - [1. Introooo](#1-introooo)
  - [2. Copy paste](#2-copy-paste)
  - [3. Do it yourself](#3-do-it-yourself)
  - [4. cloud-iniiiiiiiiiiiiit](#4-cloud-iniiiiiiiiiiiiit)

## 1. Introooo

Les fichiers Terraform pourtent l'extension `.tf` et la syntaxe utilis√©e est appel√©e HCL.

> Une √©ni√®me syntaxe pour simplement d√©clarer des cl√©s et des valeurs :)

**On appelle *Plan* un fichier Terraform qui contient des ressources √† cr√©er, √† l'aide d'un *Provider* donn√©.**

Dans notre cas, on utilisera le provider `azurerm`.

> Il vous faudra rep√©rer le "subcription ID" de votre Azure for Student. Vous pouvez le voir en saisissant "subscription" dans la barre de recherche sur l'interface Web de Azure.

Voici le minimum requis, [recommand√© dans les exemples de la doc](https://github.com/hashicorp/terraform-provider-azurerm/blob/main/website/docs/r/linux_virtual_machine.html.markdown), pour cr√©er une VM avec Azure en provider :

> En vrai ils marchaient pas ouf ou il manquait des trucs, j'vous ai m√¢ch√© le travail parce que c'est un enfer Azure hihi.

```hcl
provider "azurerm" {
  features {}
  subscription_id = "<TON_SUBSCRIPTION_ID>"
}
resource "azurerm_resource_group" "main" {
  name     = "${var.prefix}-resources"
  location = var.location
}
resource "azurerm_virtual_network" "main" {
  name                = "${var.prefix}-network"
  address_space       = ["10.0.0.0/16"]
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
}
resource "azurerm_subnet" "internal" {
  name                 = "internal"
  resource_group_name  = azurerm_resource_group.main.name
  virtual_network_name = azurerm_virtual_network.main.name
  address_prefixes     = ["10.0.2.0/24"]
}
resource "azurerm_public_ip" "pip" {
  name                = "${var.prefix}-pip"
  resource_group_name = azurerm_resource_group.main.name
  location            = azurerm_resource_group.main.location
  allocation_method   = "Static"
}
resource "azurerm_network_interface" "main" {
  name                = "${var.prefix}-nic1"
  resource_group_name = azurerm_resource_group.main.name
  location            = azurerm_resource_group.main.location
  ip_configuration {
    name                          = "primary"
    subnet_id                     = azurerm_subnet.internal.id
    private_ip_address_allocation = "Dynamic"
    public_ip_address_id          = azurerm_public_ip.pip.id
  }
}
resource "azurerm_network_interface" "internal" {
  name                = "${var.prefix}-nic2"
  resource_group_name = azurerm_resource_group.main.name
  location            = azurerm_resource_group.main.location
  ip_configuration {
    name                          = "internal"
    subnet_id                     = azurerm_subnet.internal.id
    private_ip_address_allocation = "Dynamic"
  }
}
resource "azurerm_network_security_group" "ssh" {
  name                = "ssh"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  security_rule {
    access                     = "Allow"
    direction                  = "Inbound"
    name                       = "ssh"
    priority                   = 100
    protocol                   = "Tcp"
    source_port_range          = "*"
    source_address_prefix      = "*"
    destination_port_range     = "22"
    destination_address_prefix = azurerm_network_interface.main.private_ip_address
  }
}
resource "azurerm_network_interface_security_group_association" "main" {
  network_interface_id      = azurerm_network_interface.main.id
  network_security_group_id = azurerm_network_security_group.ssh.id
}
resource "azurerm_linux_virtual_machine" "main" {
  name                            = "${var.prefix}-vm"
  resource_group_name             = azurerm_resource_group.main.name
  location                        = azurerm_resource_group.main.location
  size                            = "Standard_F2"
  admin_username                  = "<TON_USERNAME>"
  network_interface_ids = [
    azurerm_network_interface.main.id,
    azurerm_network_interface.internal.id,
  ]
  admin_ssh_key {
    username   = "<TON_USERNAME>"
    public_key = file("<CHEMIN_VERS_TA_CLE_PUBLIQUE>")
  }
  source_image_reference {
    publisher = "Canonical"
    offer     = "0001-com-ubuntu-server-jammy"
    sku       = "22_04-lts"
    version   = "latest"
  }
  os_disk {
    storage_account_type = "Standard_LRS"
    caching              = "ReadWrite"
  }
}
```

‚ûú **Remarquez dans fichier plusieurs choses**

- **le bloc `terraform {}` tout en haut du doc**
  - n√©cessaire
  - d√©finit notamment le *provider* n√©cessaire pour que l'on puisse appliquer ce *plan*
- **le bloc `provider "azurerm" {}`**
  - n√©cessaire, m√™me si non-utilis√© (comme ici)
- **les blocs `resource`**
  - sont les ressources que l'on souhaite cr√©er
  - elles sont sp√©cifiques au *provider* choisi
    - par exemple, le nom `azurerm_linux_virtual_machine` est sp√©cifique au *provider* `azurerm`
- **utilisation de variables**
  - ouais y'a un deuxi√®me fichier qui doit √™tre cr√©√© juste √† c√¥t√© en fait, `variables.tf` :

```hcl
variable "prefix" {
  description = "da prefix"
  default = "tp2magueule"
}
variable "location" {
  description = "da location"
  default = "West Europe"
}
```


## 2. Copy paste

> *Pour rappel, Terraform est √† utiliser depuis votre poste. Les fichiers √† cr√©er sont donc aussi √† cr√©er sur votre poste.*

‚ûú **Cr√©er un *plan* Terraform**

- cr√©er un nouveau r√©pertoire de travail (un dossier vide quoi, pour pas foutre le bordel j'sais pas o√π :D)
- cr√©er un fichier `main.tf`
  - dans le r√©pertoire de travail
  - remplissez-le avec le fichier d'exemple pr√©sent√© au dessus
  - remplacez les machins √† remplacer :
    - `<TON_USERNAME>`
    - `<CHEMIN_VERS_TA_CLE_PUBLIQUE>`
    - `<TON_SUBSCRIPTION_ID>`
- cr√©er aussi un fichier `variables.tf` juste √† c√¥t√©
  - pareil, avec le contenu que je vous ai fil√© au dessus

‚ûú **Depuis un shell, appliquer le plan Terraform**

- depuis un shell, se d√©placer dans le r√©pertoire de travail
- ex√©cuter les commandes suivantes :

```bash
# R√©cup√©ration du provider azurerm
$ terraform init

# V√©rification de la validit√© du plan
$ terraform plan

# D√©ploiement du plan
$ terraform apply
```

üåû **Constater le d√©ploiement**

- depuis la WebUI si tu veux
- pour le compte-rendu : depuis le CLI `az`
  - `az vm list`
  - `az vm show --name VM_NAME --resource-group RESOURCE_GROUP_NAME`
  - `az group list`
  - n'oubliez pas que vous pouvez ajouter `-o table` pour avoir un output plus lisible par un humain :)

‚ûú **Autres commandes Terraform**

```bash
# V√©rifier que votre fichier .tf est valide
$ terraform validate

# Formate un fichier .tf au format standard
$ terraform fmt

# Afficher les ressources du d√©ploiement
$ terraform state list

# Afficher les d√©tails d'une des ressources du d√©ploiement
$ terraform state show <RESSOURCE>

# D√©truit les ressources d√©ploy√©es
$ terraform destroy
```

## 3. Do it yourself

üåû **Cr√©er un *plan Terraform* avec les contraintes suivantes**

- `node1`
  - Ubuntu 22.04
  - 1 IP Publique
  - 1 IP Priv√©e
- `node2`
  - Ubuntu 22.04
  - 1 IP Priv√©e
- les IPs priv√©es doivent permettre aux deux machines de se `ping`

> Pour acc√©der √† `node2`, il faut donc d'abord se connecter √† `node1`, et effectuer une connexion SSH vers `node2`. Vous pouvez ajouter l'option `-j` de SSH pour faire ~~des dingueries~~ un rebond SSH (`-j` comme Jump). `ssh -j node1 node2` vous connectera √† `node2` en passant par `node1`.

## 4. cloud-iniiiiiiiiiiiiit

üåû **Int√©grer la gestion de `cloud-init`**

- faire pop une VM Ubuntu 22.04 qui utilise `cloud-init` au premier boot
- ce `cloud-init` doit faire pareil qu'√† la partie pr√©c√©dente :
  - installer Docker sur la machine
  - ajoutez un user qui porte votre pseudo
    - il a un password d√©fini
    - cl√© SSH publique d√©pos√©e
    - membre du groupe `docker`
  - l'image Docker `alpine:latest` doit √™tre t√©l√©charg√©e

üåû **Moar `cloud-init` and Terraform configuration**

- adaptez le plan Terraform et le fichier `cloud-init` pr√©c√©dents
- un `docker-compose.yml` est automatiquement d√©pos√©
  - il se situe dans `/opt/wikijs`
  - c'est le `docker-compose.yml` de la doc officielle de WikiJS (pareil qu'au TP1)
- le `docker-compose.yml` est automatiquement d√©marr√©
- par d√©faut, WikiJS √©coute sur le port 3000
  - vous devrez modifier le `docker-compose.yml` pour que ce port soit partag√© sur le port 10101 de la machine h√¥te
- le Network Security Group associ√© √† l'interface autorise le traffic sur le port 10101 sp√©cifiquement

üåû **Play with compose**

- ajoutez dans le `docker-compose.yml` un troisi√®me conteneur NGINX
  - agit comme un reverse proxy pour acc√©der au WikiJS
  - uniquement une connexion chiffr√©e avec TLS (`https://` quoi)
    - g√©n√©rez un certificat auto-sign√© pour √ßa
  - il √©coute sur le port 443 dans le conteneur (la conf NGINX)
  - il est dispo sur le port 443 de l'h√¥te (partage de port Docker)
- ce conteneur reverse-proxy est le **seul** moyen d'acc√©der √† WikiJS
  - il faut enlever le partage de port du conteneur WikiJS
- ajouter des `healthcheck` dans le `docker-compose.yml`
  - une m√©canique pour qu'un conteneur soit d√©termin√© comme en bonne sant√© quand il valide une commande
  - le conteneur NGINX doit √™tre marqu√© comme *healthy* quand son port 443 est joignable
  - le conteneur WikiJS doit √™tre marqu√© comme *healthy* quand son port 3000 est joignable
- ajouter un `depends_on` pour forcer NGINX √† d√©marrer apr√®s WikiJS

![Terraforming Mars](./img/terraforming_mars.jpg)
